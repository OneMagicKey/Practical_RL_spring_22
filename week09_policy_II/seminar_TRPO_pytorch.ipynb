{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TvSJpmxbOslr","executionInfo":{"status":"ok","timestamp":1648717703801,"user_tz":-180,"elapsed":25172,"user":{"displayName":"Даниил Толстых","userId":"13751994581296830682"}},"outputId":"a6e9a3f9-25ab-4c1b-f24f-79c3a18ae36b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Selecting previously unselected package xvfb.\n","(Reading database ... 156210 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Starting virtual X frame buffer: Xvfb.\n"]}],"source":["import sys, os\n","if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n","    !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"]},{"cell_type":"markdown","metadata":{"id":"DoNL1kWCOslw"},"source":["### Let's make a TRPO!\n","\n","In this notebook we will write the code of the one Trust Region Policy Optimization.\n","As usually, it contains a few different parts which we are going to reproduce.\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"RE4u83CGOsly","executionInfo":{"status":"ok","timestamp":1648717711369,"user_tz":-180,"elapsed":7577,"user":{"displayName":"Даниил Толстых","userId":"13751994581296830682"}}},"outputs":[],"source":["from typing import Tuple\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"owlxG5ilOslz","executionInfo":{"status":"ok","timestamp":1648717711774,"user_tz":-180,"elapsed":417,"user":{"displayName":"Даниил Толстых","userId":"13751994581296830682"}},"outputId":"bf0ef609-b91b-4511-e1c4-645f1fca782a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Observation Space Box(-28.274333953857422, 28.274333953857422, (6,), float32)\n","Action Space Discrete(3)\n"]}],"source":["import gym\n","\n","env = gym.make(\"Acrobot-v1\")\n","env.reset()\n","observation_shape = env.observation_space.shape\n","n_actions = env.action_space.n\n","\n","print(\"Observation Space\", env.observation_space)\n","print(\"Action Space\", env.action_space)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":517},"id":"CmRWE-djOslz","executionInfo":{"status":"ok","timestamp":1648717713435,"user_tz":-180,"elapsed":1665,"user":{"displayName":"Даниил Толстых","userId":"13751994581296830682"}},"outputId":"bf3e433b-a91f-4a83-84c3-ef14a7372dd3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<PIL.Image.Image image mode=RGB size=500x500 at 0x7FC8518E0410>"],"image/png":"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\n"},"metadata":{},"execution_count":4}],"source":["from PIL import Image\n","\n","Image.fromarray(env.render('rgb_array'))"]},{"cell_type":"markdown","metadata":{"id":"_5xWfXmCOsl0"},"source":["### Step 1: Defining a network\n","\n","With all it's complexity, at it's core TRPO is yet another policy gradient method.\n","\n","This essentially means we're actually training a stochastic policy $\\pi_\\theta \\left( a \\middle| s \\right)$.\n","\n","And yes, it's gonna be a neural network. So let's start by defining one."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"swjqJILiOsl0","executionInfo":{"status":"ok","timestamp":1648717941193,"user_tz":-180,"elapsed":353,"user":{"displayName":"Даниил Толстых","userId":"13751994581296830682"}}},"outputs":[],"source":["class TRPOAgent(nn.Module):\n","    def __init__(self, state_shape: Tuple[int], n_actions: int):\n","        '''\n","        Here you should define your model\n","        You should have LOG-PROBABILITIES as output because you will need it to compute loss\n","        We recommend that you start simple:\n","        use 1-2 hidden layers with 100-500 units and relu for the first try\n","        '''\n","        nn.Module.__init__(self)\n","\n","        assert isinstance(state_shape, tuple)\n","        assert len(state_shape) == 1\n","        input_dim = state_shape[0]\n","        \n","        # Prepare your model here.\n","        self.model = nn.Sequential(nn.Linear(state_shape[0], 256),\n","                                   nn.ReLU(),\n","                                   nn.Linear(256, n_actions),\n","                                   nn.LogSoftmax())\n","\n","    def forward(self, states: torch.Tensor):\n","        \"\"\"\n","        takes agent's observation, returns log-probabilities\n","        :param state_t: a batch of states, shape = [batch_size, state_shape]\n","        \"\"\"\n","\n","        # Use your network to compute log_probs for the given states.\n","        log_probs = self.model(states)\n","        \n","        return log_probs\n","\n","    def get_log_probs(self, states: torch.Tensor):\n","        '''\n","        Log-probs for training\n","        '''\n","        return self.forward(states)\n","\n","    def get_probs(self, states: torch.Tensor):\n","        '''\n","        Probs for interaction\n","        '''\n","        return torch.exp(self.forward(states))\n","\n","    def act(self, obs: np.ndarray, sample: bool = True):\n","        '''\n","        Samples action from policy distribution (sample = True) or takes most likely action (sample = False)\n","        :param: obs - single observation vector\n","        :param sample: if True, samples from \\pi, otherwise takes most likely action\n","        :returns: action (single integer) and probabilities for all actions\n","        '''\n","\n","        with torch.no_grad():\n","            probs = self.get_probs(torch.tensor(obs[np.newaxis], dtype=torch.float32)).numpy()\n","\n","        if sample:\n","            action = int(np.random.choice(n_actions, p=probs[0]))\n","        else:\n","            action = int(np.argmax(probs))\n","\n","        return action, probs[0]\n","\n","\n","agent = TRPOAgent(observation_shape, n_actions)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vu7HFyOhOsl2","executionInfo":{"status":"ok","timestamp":1648717943095,"user_tz":-180,"elapsed":388,"user":{"displayName":"Даниил Толстых","userId":"13751994581296830682"}},"outputId":"d5e0c1bb-ed52-4c23-cea1-6d703f6d6e6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["sampled: [(2, array([0.3526425 , 0.31731975, 0.33003777], dtype=float32)), (2, array([0.35258543, 0.3133252 , 0.33408946], dtype=float32)), (2, array([0.3534529 , 0.31249902, 0.33404803], dtype=float32)), (2, array([0.35361844, 0.3182642 , 0.32811737], dtype=float32)), (0, array([0.35273924, 0.3169638 , 0.33029696], dtype=float32))]\n","greedy: [(0, array([0.35253575, 0.31429937, 0.33316487], dtype=float32)), (0, array([0.35634044, 0.31497264, 0.32868692], dtype=float32)), (0, array([0.35413244, 0.3145219 , 0.33134565], dtype=float32)), (0, array([0.35515594, 0.31508774, 0.32975635], dtype=float32)), (0, array([0.3526011, 0.3188882, 0.3285107], dtype=float32))]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"]}],"source":["# Check if log-probabilities satisfies all the requirements\n","log_probs = agent.get_log_probs(torch.tensor(env.reset()[np.newaxis], dtype=torch.float32))\n","assert (\n","    isinstance(log_probs, torch.Tensor) and\n","    log_probs.requires_grad\n","), \"log_probs must be a torch.Tensor with grad\"\n","assert log_probs.shape == (1, n_actions)\n","sums = torch.exp(log_probs).sum(dim=1)\n","assert torch.allclose(sums, torch.ones_like(sums))\n","\n","# Demo use\n","print(\"sampled:\", [agent.act(env.reset()) for _ in range(5)])\n","print(\"greedy:\", [agent.act(env.reset(), sample=False) for _ in range(5)])"]},{"cell_type":"markdown","metadata":{"id":"6gnltkJwOsl3"},"source":["#### Flat parameters operations\n","\n","We are going to use it"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"60kvTKzzOsl4","executionInfo":{"status":"ok","timestamp":1648717975491,"user_tz":-180,"elapsed":462,"user":{"displayName":"Даниил Толстых","userId":"13751994581296830682"}}},"outputs":[],"source":["def get_flat_params_from(model):\n","    params = [torch.ravel(param.detach()) for param in model.parameters()]\n","    flat_params = torch.cat(params)\n","    return flat_params\n","\n","\n","def set_flat_params_to(model, flat_params):\n","    prev_ind = 0\n","    for param in model.parameters():\n","        flat_size = int(np.prod(list(param.shape)))\n","        param.data.copy_(\n","            flat_params[prev_ind:prev_ind + flat_size].reshape(param.shape)\n","        )\n","        prev_ind += flat_size"]},{"cell_type":"markdown","metadata":{"id":"5vzAxOtZOsl4"},"source":["Compute cumulative reward just like you did in vanilla REINFORCE"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"8FDpdg95Osl5","executionInfo":{"status":"ok","timestamp":1648717986077,"user_tz":-180,"elapsed":796,"user":{"displayName":"Даниил Толстых","userId":"13751994581296830682"}}},"outputs":[],"source":["import scipy.signal\n","\n","\n","def get_cumulative_returns(r, gamma=1):\n","    \"\"\"\n","    Computes cumulative discounted rewards given immediate rewards\n","    G_i = r_i + gamma*r_{i+1} + gamma^2*r_{i+2} + ...\n","    Also known as R(s,a).\n","    \"\"\"\n","    r = np.array(r)\n","    assert r.ndim >= 1\n","    return scipy.signal.lfilter([1], [1, -gamma], r[::-1], axis=0)[::-1]"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V6kl245pOsl5","executionInfo":{"status":"ok","timestamp":1648718012008,"user_tz":-180,"elapsed":376,"user":{"displayName":"Даниил Толстых","userId":"13751994581296830682"}},"outputId":"76580c76-99a3-4604-e473-873c75d77679"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.40049, 1.5561 , 1.729  , 0.81   , 0.9    , 1.     ])"]},"metadata":{},"execution_count":9}],"source":["# simple demo on rewards [0,0,1,0,0,1]\n","get_cumulative_returns([0, 0, 1, 0, 0, 1], gamma=0.9)"]},{"cell_type":"markdown","metadata":{"id":"wfZ9pBpBOsl5"},"source":["**Rollout**"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"rJFZrcsaOsl6","executionInfo":{"status":"ok","timestamp":1648718030232,"user_tz":-180,"elapsed":363,"user":{"displayName":"Даниил Толстых","userId":"13751994581296830682"}}},"outputs":[],"source":["def rollout(env, agent, max_pathlength=2500, n_timesteps=50000):\n","    \"\"\"\n","    Generate rollouts for training.\n","    :param: env - environment in which we will make actions to generate rollouts.\n","    :param: act - the function that can return policy and action given observation.\n","    :param: max_pathlength - maximum size of one path that we generate.\n","    :param: n_timesteps - total sum of sizes of all pathes we generate.\n","    \"\"\"\n","    paths = []\n","\n","    total_timesteps = 0\n","    while total_timesteps < n_timesteps:\n","        obervations, actions, rewards, action_probs = [], [], [], []\n","        obervation = env.reset()\n","        for _ in range(max_pathlength):\n","            action, policy = agent.act(obervation)\n","            obervations.append(obervation)\n","            actions.append(action)\n","            action_probs.append(policy)\n","            obervation, reward, done, _ = env.step(action)\n","            rewards.append(reward)\n","            total_timesteps += 1\n","            if done or total_timesteps >= n_timesteps:\n","                path = {\n","                    \"observations\": np.array(obervations),\n","                    \"policy\": np.array(action_probs),\n","                    \"actions\": np.array(actions),\n","                    \"rewards\": np.array(rewards),\n","                    \"cumulative_returns\": get_cumulative_returns(rewards),\n","                }\n","                paths.append(path)\n","                break\n","    return paths"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TvV5pS1xOsl6","executionInfo":{"status":"ok","timestamp":1648718032924,"user_tz":-180,"elapsed":6,"user":{"displayName":"Даниил Толстых","userId":"13751994581296830682"}},"outputId":"cc25eb22-e151-4a49-fbd2-e3ccd965e109"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'actions': array([1, 2, 0, 0, 0]),\n"," 'cumulative_returns': array([-5., -4., -3., -2., -1.]),\n"," 'observations': array([[ 0.99769351,  0.06787975,  0.9954736 , -0.09503846, -0.03942875,\n","         0.00211655],\n","       [ 0.99873058,  0.05037093,  0.99688068, -0.07892345, -0.13180761,\n","         0.15400293],\n","       [ 0.99998991,  0.00449302,  0.99999528, -0.00307236, -0.31530473,\n","         0.58476639],\n","       [ 0.99909498, -0.04253484,  0.99724819,  0.0741353 , -0.14347418,\n","         0.16800736],\n","       [ 0.99874677, -0.05004885,  0.99820519,  0.05988663,  0.06906215,\n","        -0.30897957]]),\n"," 'policy': array([[0.35222572, 0.3171219 , 0.33065233],\n","       [0.35091946, 0.31681374, 0.33226675],\n","       [0.3444356 , 0.3148964 , 0.34066796],\n","       [0.35414034, 0.3144831 , 0.33137658],\n","       [0.35564685, 0.31782308, 0.3265301 ]], dtype=float32),\n"," 'rewards': array([-1., -1., -1., -1., -1.])}\n","It's ok\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"]}],"source":["from pprint import pprint\n","\n","paths = rollout(env, agent, max_pathlength=5, n_timesteps=100)\n","pprint(paths[-1])\n","\n","assert (paths[0]['policy'].shape == (5, n_actions))\n","assert (paths[0]['cumulative_returns'].shape == (5,))\n","assert (paths[0]['rewards'].shape == (5,))\n","assert (paths[0]['observations'].shape == (5,) + observation_shape)\n","assert (paths[0]['actions'].shape == (5,))\n","\n","print(\"It's ok\")"]},{"cell_type":"markdown","metadata":{"id":"HsKfCTwgOsl6"},"source":["### Step 3: Auxiliary functions\n","\n","Now let's define the loss functions and something else for actual TRPO training."]},{"cell_type":"markdown","metadata":{"id":"zkmd8-jPOsl7"},"source":["The surrogate reward should be:\n","$$J_{surr}= {1 \\over N} \\sum\\limits_{i=1}^N \\frac{\\pi_{\\theta}(s_i, a_i)}{\\pi_{\\theta_{old}}(s_i, a_i)}A_{\\theta_{old}(s_i, a_i)}$$\n","\n","For simplicity, in this assignment we are going to use cumulative rewards instead of advantage:\n","$$J'_{surr}= {1 \\over N} \\sum\\limits_{i=1}^N \\frac{\\pi_{\\theta}(s_i, a_i)}{\\pi_{\\theta_{old}}(s_i, a_i)}G_{\\theta_{old}(s_i, a_i)}$$\n","\n","Since we want to maximize the reward, we are going to minimize the corresponding surrogate loss:\n","$$ L_{surr} = - J'_{surr} $$\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"USNuzsw2Osl7","executionInfo":{"status":"ok","timestamp":1648718281548,"user_tz":-180,"elapsed":6,"user":{"displayName":"Даниил Толстых","userId":"13751994581296830682"}}},"outputs":[],"source":["def get_loss(agent, observations, actions, cumulative_returns, old_probs):\n","    \"\"\"\n","    Computes TRPO objective\n","    :param: observations - batch of observations [timesteps x state_shape]\n","    :param: actions - batch of actions [timesteps]\n","    :param: cumulative_returns - batch of cumulative returns [timesteps]\n","    :param: old_probs - batch of probabilities computed by old network [timesteps x num_actions]\n","    :returns: scalar value of the objective function\n","    \"\"\"\n","    batch_size = observations.shape[0]\n","    probs_all = agent.get_probs(observations)\n","\n","    probs_for_actions = probs_all[torch.arange(batch_size), actions]\n","    old_probs_for_actions = old_probs[torch.arange(batch_size), actions]\n","\n","    # Compute surrogate loss, aka importance-sampled policy gradient\n","    loss = torch.mean(cumulative_returns * (probs_for_actions / old_probs_for_actions))\n","\n","    assert loss.ndim == 0\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"R0kCiNVbOsl7"},"source":["We can ascend these gradients as long as our $\\pi_\\theta(a|s)$ satisfies the constraint\n","$$\\mathbb{E}_{s,\\pi_{\\theta_{t}}} \\Big[ \\operatorname{KL} \\left( \\pi_{\\theta_{t}} (s) \\:\\|\\: \\pi_{\\theta_{t+1}} (s) \\right) \\Big] < \\alpha$$\n","\n","\n","where\n","\n","$$\\operatorname{KL} \\left( p \\| q \\right) = \\mathbb{E}_p \\log \\left( \\frac p q \\right)$$"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"G8OqNEL9Osl7","executionInfo":{"status":"ok","timestamp":1648718698538,"user_tz":-180,"elapsed":270,"user":{"displayName":"Даниил Толстых","userId":"13751994581296830682"}}},"outputs":[],"source":["def get_kl(agent, observations, actions, cumulative_returns, old_probs):\n","    \"\"\"\n","    Computes KL-divergence between network policy and old policy\n","    :param: observations - batch of observations [timesteps x state_shape]\n","    :param: actions - batch of actions [timesteps]\n","    :param: cumulative_returns - batch of cumulative returns [timesteps] (we don't need it actually)\n","    :param: old_probs - batch of probabilities computed by old network [timesteps x num_actions]\n","    :returns: scalar value of the KL-divergence\n","    \"\"\"\n","    batch_size = observations.shape[0]\n","    log_probs_all = agent.get_log_probs(observations)\n","    probs_all = torch.exp(log_probs_all)\n","\n","    # Compute Kullback-Leibler divergence (see formula above).\n","    # Note: you need to sum KL and entropy over all actions, not just the ones agent took.\n","    # You will also need to compute max KL over all timesteps.\n","    old_log_probs = torch.log(old_probs + 1e-10)\n","\n","    kl = torch.mean(old_log_probs * (old_log_probs - log_probs_all))\n","\n","    assert kl.ndim == 0\n","    assert (kl > -0.0001).all() and (kl < 10000).all()\n","    return kl"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"SXR_uN-WOsl8","executionInfo":{"status":"ok","timestamp":1648718700853,"user_tz":-180,"elapsed":289,"user":{"displayName":"Даниил Толстых","userId":"13751994581296830682"}}},"outputs":[],"source":["def get_entropy(agent, observations):\n","    \"\"\"\n","    Computes entropy of the network policy\n","    :param: observations - batch of observations\n","    :returns: scalar value of the entropy\n","    \"\"\"\n","\n","    observations = torch.tensor(observations, dtype=torch.float32)\n","\n","    log_probs_all = agent.get_log_probs(observations)\n","    probs_all = torch.exp(log_probs_all)\n","\n","    entropy = (-probs_all * log_probs_all).sum(dim=1).mean(dim=0)\n","\n","    assert entropy.ndim == 0\n","    return entropy"]},{"cell_type":"markdown","metadata":{"id":"oS7KtxcwOsl8"},"source":["**Linear search**\n","\n","TRPO in its core involves ascending surrogate policy gradient constrained by KL divergence.\n","\n","In order to enforce this constraint, we're gonna use linesearch. You can find out more about it [here](https://en.wikipedia.org/wiki/Linear_search)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tgLp_f7Osl8"},"outputs":[],"source":["def linesearch(f, x: torch.Tensor, fullstep: torch.Tensor, max_kl: float, max_backtracks: int = 10, backtrack_coef: float = 0.5):\n","    \"\"\"\n","    Linesearch finds the best parameters of neural networks in the direction of fullstep contrainted by KL divergence.\n","    :param: f - function that returns loss, kl and arbitrary third component.\n","    :param: x - old parameters of neural network.\n","    :param: fullstep - direction in which we make search.\n","    :param: max_kl - constraint of KL divergence.\n","    :returns:\n","    \"\"\"\n","    loss, _, = f(x)\n","    for stepfrac in backtrack_coef**np.arange(max_backtracks):\n","        xnew = x + stepfrac * fullstep\n","        new_loss, kl = f(xnew)\n","        if kl <= max_kl and new_loss < loss:\n","            x = xnew\n","            loss = new_loss\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"kjaqGG1iOsl9"},"source":["**Conjugate gradients**\n","\n","Since TRPO includes contrainted optimization, we will need to solve $A x = b$ using conjugate gradients.\n","\n","In general, CG is an algorithm that solves $A x = b$ where $A$ is positive-defined. $A$ is the Hessian matrix so $A$ is positive-defined. You can find out more about CG [here](https://en.wikipedia.org/wiki/Conjugate_gradient_method)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybeDVDcbOsl9"},"outputs":[],"source":["def conjugate_gradient(f_Ax, b, cg_iters=10, residual_tol=1e-10):\n","    \"\"\"\n","    This method solves system of equation Ax=b using an iterative method called conjugate gradients\n","    :f_Ax: function that returns Ax\n","    :b: targets for Ax\n","    :cg_iters: how many iterations this method should do\n","    :residual_tol: epsilon for stability\n","    \"\"\"\n","    p = b.clone()\n","    r = b.clone()\n","    x = torch.zeros_like(b)\n","    rdotr = torch.sum(r*r)\n","    for i in range(cg_iters):\n","        z = f_Ax(p)\n","        v = rdotr / (torch.sum(p*z) + 1e-8)\n","        x += v * p\n","        r -= v * z\n","        newrdotr = torch.sum(r*r)\n","        mu = newrdotr / (rdotr + 1e-8)\n","        p = r + mu * p\n","        rdotr = newrdotr\n","        if rdotr < residual_tol:\n","            break\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcgYZEkDOsl9"},"outputs":[],"source":["# This code validates conjugate gradients\n","A = np.random.rand(8, 8)\n","A = A.T @ A\n","\n","\n","def f_Ax(x):\n","    return torch.ravel(torch.tensor(A, dtype=torch.float32) @ x.reshape(-1, 1))\n","\n","\n","b = np.random.rand(8)\n","w = (np.linalg.inv(A.T @ A) @ A.T @ b.reshape(-1, 1)).reshape(-1)\n","\n","print(w)\n","print(conjugate_gradient(f_Ax, torch.tensor(b, dtype=torch.float32)).numpy())"]},{"cell_type":"markdown","metadata":{"id":"wwcQHdxsOsl9"},"source":["### Step 4: training\n","In this section we construct the whole update step function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0IOtNCIOsl9"},"outputs":[],"source":["def update_step(agent, observations, actions, cumulative_returns, old_probs, max_kl):\n","    \"\"\"\n","    This function does the TRPO update step\n","    :param: observations - batch of observations\n","    :param: actions - batch of actions\n","    :param: cumulative_returns - batch of cumulative returns\n","    :param: old_probs - batch of probabilities computed by old network\n","    :param: max_kl - controls how big KL divergence may be between old and new policy every step.\n","    :returns: KL between new and old policies and the value of the loss function.\n","    \"\"\"\n","\n","    # Here we prepare the information\n","    observations = torch.tensor(observations, dtype=torch.float32)\n","    actions = torch.tensor(actions, dtype=torch.int64)\n","    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n","    old_probs = torch.tensor(old_probs, dtype=torch.float32)\n","\n","    # Here we compute gradient of the loss function\n","    loss = get_loss(agent, observations, actions, cumulative_returns, old_probs)\n","    grads = torch.autograd.grad(loss, agent.parameters())\n","    loss_grad = torch.cat([torch.ravel(grad.detach()) for grad in grads])\n","\n","    def Fvp(v):\n","        # Here we compute Fx to do solve Fx = g using conjugate gradients\n","        # We actually do here a couple of tricks to compute it efficiently\n","\n","        kl = get_kl(agent, observations, actions, cumulative_returns, old_probs)\n","\n","        grads = torch.autograd.grad(kl, agent.parameters(), create_graph=True)\n","        flat_grad_kl = torch.cat([grad.reshape(-1) for grad in grads])\n","\n","        kl_v = (flat_grad_kl * v).sum()\n","        grads = torch.autograd.grad(kl_v, agent.parameters())\n","        flat_grad_grad_kl = torch.cat([torch.ravel(grad) for grad in grads]).detach()\n","\n","        return flat_grad_grad_kl + v * 0.1\n","\n","    # Here we solve Fx = g system using conjugate gradients\n","    stepdir = conjugate_gradient(Fvp, -loss_grad, 10)\n","\n","    # Here we compute the initial vector to do linear search\n","    shs = 0.5 * (stepdir * Fvp(stepdir)).sum(0, keepdim=True)\n","\n","    lm = torch.sqrt(shs / max_kl)\n","    fullstep = stepdir / lm[0]\n","\n","    neggdotstepdir = (-loss_grad * stepdir).sum(0, keepdim=True)\n","\n","    # Here we get the start point\n","    prev_params = get_flat_params_from(agent)\n","\n","    def get_loss_kl(params):\n","        # Helper for linear search\n","        set_flat_params_to(agent, params)\n","        return [\n","            get_loss(agent, observations, actions, cumulative_returns, old_probs),\n","            get_kl(agent, observations, actions, cumulative_returns, old_probs),\n","        ]\n","\n","    # Here we find our new parameters\n","    new_params = linesearch(get_loss_kl, prev_params, fullstep, max_kl)\n","\n","    # And we set it to our network\n","    set_flat_params_to(agent, new_params)\n","\n","    return get_loss_kl(new_params)"]},{"cell_type":"markdown","metadata":{"id":"ORNkns4WOsl-"},"source":["### Step 5: Main TRPO loop\n","\n","Here we will train our network!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJqLnOp3Osl-"},"outputs":[],"source":["import time\n","from itertools import count\n","\n","# TRPO hyperparameter; controls how big KL divergence may be between the old and the new policy at every step.\n","max_kl = 0.01\n","numeptotal = 0  # Number of episodes we have completed so far.\n","\n","start_time = time.time()\n","\n","for i in count(1):\n","    print(\"\\n********** Iteration %i ************\" % i)\n","\n","    # Generating paths.\n","    print(\"Rollout\")\n","    paths = rollout(env, agent)\n","    print(\"Made rollout\")\n","\n","    # Updating policy.\n","    observations = np.concatenate([path[\"observations\"] for path in paths])\n","    actions = np.concatenate([path[\"actions\"] for path in paths])\n","    returns = np.concatenate([path[\"cumulative_returns\"] for path in paths])\n","    old_probs = np.concatenate([path[\"policy\"] for path in paths])\n","\n","    loss, kl = update_step(agent, observations, actions, returns, old_probs, max_kl)\n","\n","    # Report current progress\n","    episode_rewards = np.array([path[\"rewards\"].sum() for path in paths])\n","\n","    stats = {}\n","    numeptotal += len(episode_rewards)\n","    stats[\"Total number of episodes\"] = numeptotal\n","    stats[\"Average sum of rewards per episode\"] = episode_rewards.mean()\n","    stats[\"Std of rewards per episode\"] = episode_rewards.std()\n","    stats[\"Time elapsed\"] = \"%.2f mins\" % ((time.time() - start_time)/60.)\n","    stats[\"KL between old and new distribution\"] = kl.data.numpy()\n","    stats[\"Entropy\"] = get_entropy(agent, observations).data.numpy()\n","    stats[\"Surrogate loss\"] = loss.data.numpy()\n","    for k, v in stats.items():\n","        print(k + \": \" + \" \" * (40 - len(k)) + str(v))"]},{"cell_type":"markdown","metadata":{"id":"uH4X2LXlOsl-"},"source":["# Homework option I: better sampling (10+pts)\n","\n","In this section, you're invited to implement a better rollout strategy called _vine_.\n","\n","![img](https://s17.postimg.cc/i90chxgvj/vine.png)\n","\n","In most gym environments, you can actually backtrack by using states. You can find a wrapper that saves/loads states in [the MCTS seminar](https://github.com/yandexdataschool/Practical_RL/blob/master/week10_planning/seminar_MCTS.ipynb).\n","\n","You can read more about TRPO in the [original paper](https://arxiv.org/abs/1502.05477) in section 5.2.\n","\n","The goal here is to implement such rollout policy (we recommend using tree data structure like in the seminar above).\n","Then you can assign cumulative rewards similar to `get_cumulative_rewards`, but for a tree.\n","\n","__bonus task__ - parallelize samples using multiple cores"]},{"cell_type":"markdown","metadata":{"id":"dze2Jf28Osl-"},"source":["# Homework option II (10+pts)\n","\n","Let's use TRPO to train evil robots! (pick any of two)\n","* [MuJoCo robots](https://gym.openai.com/envs#mujoco)\n","* [Box2d robot](https://gym.openai.com/envs/BipedalWalker-v2)\n","\n","The catch here is that those environments have continuous action spaces.\n","\n","Luckily, TRPO is a policy gradient method, so it's gonna work for any parametric $\\pi_\\theta(a|s)$. We recommend starting with gaussian policy:\n","\n","$$\\pi_\\theta(a|s) = N(\\mu_\\theta(s),\\sigma^2_\\theta(s)) = {1 \\over \\sqrt { 2 \\pi {\\sigma^2}_\\theta(s) } } e^{ (a -\n","\\mu_\\theta(s))^2 \\over 2 {\\sigma^2}_\\theta(s) } $$\n","\n","In the $\\sqrt { 2 \\pi {\\sigma^2}_\\theta(s) }$ clause, $\\pi$ means ~3.1415926, not agent's policy.\n","\n","This essentially means that you will need two output layers:\n","* $\\mu_\\theta(s)$, a dense layer with linear activation\n","* ${\\sigma^2}_\\theta(s)$, a dense layer with activation tf.exp (to make it positive; like rho from bandits)\n","\n","For multidimensional actions, you can use a fully factorized gaussian (basically a vector of gaussians).\n","\n","__Bonus task__: compare the performance of the continuous action space method to action space discretization."]}],"metadata":{"language_info":{"name":"python"},"colab":{"name":"seminar_TRPO_pytorch.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}